<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>神经网络汇总 | 纸箱小站</title><meta name="description" content="神经网络从入门到神经!"><meta name="keywords" content="神经网络"><meta name="author" content="熊猫熬夜不伤神"><meta name="copyright" content="熊猫熬夜不伤神"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2020/02/04/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="神经网络汇总"><meta property="og:url" content="http://yoursite.com/2020/02/04/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><meta property="og:site_name" content="纸箱小站"><meta property="og:description" content="神经网络从入门到神经!"><meta property="og:image" content="https://ftp.bmp.ovh/imgs/2020/07/010ccb989ca14eeb.jpg"><meta property="article:published_time" content="2020-02-03T16:00:00.000Z"><meta property="article:modified_time" content="2020-02-23T16:00:00.000Z"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="Git汇总" href="http://yoursite.com/2020/02/10/Git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="next" title="markdown试水" href="http://yoursite.com/2020/01/28/hello%20typora/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://ftp.bmp.ovh/imgs/2020/07/9399385b6f3d4914.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">9</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#gradient-descent-gt-the-way-we-use-to-reduce-the-value-of-cost-function"><span class="toc-number">1.</span> <span class="toc-text">gradient descent -&gt; the way we use to reduce the value of cost function</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#当c只由两个变量组成时："><span class="toc-number">1.1.</span> <span class="toc-text">当c只由两个变量组成时：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#当c由多个变量组成时"><span class="toc-number">1.1.1.</span> <span class="toc-text">当c由多个变量组成时</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#操作方法"><span class="toc-number">2.</span> <span class="toc-text">操作方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#什么是端到端学习？"><span class="toc-number">3.</span> <span class="toc-text">什么是端到端学习？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Softmax回归介绍"><span class="toc-number">4.</span> <span class="toc-text">Softmax回归介绍</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://ftp.bmp.ovh/imgs/2020/07/010ccb989ca14eeb.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">纸箱小站</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">神经网络汇总</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-02-04 00:00:00"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-02-04</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-02-24 00:00:00"><i class="fas fa-history fa-fw"></i> 更新于 2020-02-24</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><ul>
<li><p>如何对图像进行分割？</p>
<blockquote>
<p>There are many approaches to solving the segmentation problem. One approach is to trial many different ways of segmenting the image, using the individual digit classifier to score each trial segmentation. A trial segmentation gets a high score if the individual digit classifier is confident of its classification in all segments, and a low score if the classifier is having a lot of trouble in one or more segments. The idea is that if the classifier is having trouble somewhere, then it’s probably having trouble because the segmentation has been chosen incorrectly. This idea and other variations can be used to solve the segmentation problem quite well. So instead of worrying about segmentation we’ll concentrate on developing a neural network which can solve the more interesting and difficult problem, namely, recognizing individual handwritten digits.<br>即不需要在这方面过多劳神，我们通过比较试验结果选出最合适的分割方法</p>
</blockquote>
</li>
</ul>
<hr>
<p>  <img src= "/img/loading.gif" data-src="http://neuralnetworksanddeeplearning.com/images/tikz12.png" alt="img"></p>
<blockquote>
<p>The input pixels are greyscale, with a value of 0.00.0 representing white, a value of 1.01.0 representing black, and in between values representing gradually darkening shades of grey.</p>
</blockquote>
<ul>
<li><p>为什么选择10个而不是4个（2进制）输出？</p>
<blockquote>
<p>To understand why we do this, it helps to think about what the neural network is doing from first principles. Consider first the case where we use 1010 output neurons. Let’s concentrate on the first output neuron, the one that’s trying to decide whether or not the digit is a 00. It does this by weighing up evidence from the hidden layer of neurons. What are those hidden neurons doing? Well, just suppose for the sake of argument that the first neuron in the hidden layer detects whether or not an image like the following is present:<br><img src= "/img/loading.gif" data-src="http://neuralnetworksanddeeplearning.com/images/mnist_top_left_feature.png" alt="img"><br>It can do this by heavily weighting input pixels which overlap with the image, and only lightly weighting the other inputs. In a similar way, let’s suppose for the sake of argument that the second, third, and fourth neurons in the hidden layer detect whether or not the following images are present:<br><img src= "/img/loading.gif" data-src="http://neuralnetworksanddeeplearning.com/images/mnist_other_features.png" alt="img"><br>As you may have guessed, these four images together make up the 00 image that we saw in the line of digits shown <a href="http://neuralnetworksanddeeplearning.com/chap1.html#complete_zero" target="_blank" rel="noopener">earlier</a>:<br><img src= "/img/loading.gif" data-src="http://neuralnetworksanddeeplearning.com/images/mnist_complete_zero.png" alt="img"><br>So if all four of these hidden neurons are firing then we can conclude that the digit is a 00. Of course, that’s not the <em>only</em> sort of evidence we can use to conclude that the image was a 00 - we could legitimately get a 00 in many other ways (say, through translations of the above images, or slight distortions). But it seems safe to say that at least in this case we’d conclude that the input was a 00.<br>Supposing the neural network functions in this way, we can give a plausible explanation for why it’s better to have 1010 outputs from the network, rather than 44. If we had 44 outputs, then the first output neuron would be trying to decide what the most significant bit of the digit was. And there’s no easy way to relate that most significant bit to simple shapes like those shown above. It’s hard to imagine that there’s any good historical reason the component shapes of the digit will be closely related to (say) the most significant bit in the output.<br>Now, with all that said, this is all just a heuristic. Nothing says that the three-layer neural network has to operate in the way I described, with the hidden neurons detecting simple component shapes. Maybe a clever learning algorithm will find some assignment of weights that lets us use only 44 output neurons. But as a heuristic the way of thinking I’ve described works pretty well, and can save you a lot of time in designing good neural network architectures.</p>
</blockquote>
</li>
<li><p>cost function == loss function == objective function</p>
</li>
</ul>
<h1 id="gradient-descent-gt-the-way-we-use-to-reduce-the-value-of-cost-function"><a href="#gradient-descent-gt-the-way-we-use-to-reduce-the-value-of-cost-function" class="headerlink" title="gradient descent -&gt; the way we use to reduce the value of cost function"></a>gradient descent <em>-&gt; the way we use to reduce the value of cost function</em></h1><h2 id="当c只由两个变量组成时："><a href="#当c只由两个变量组成时：" class="headerlink" title="当c只由两个变量组成时："></a>当c只由两个变量组成时：</h2><p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208110006272.png" alt="image-20200208110006272"></p>
<p>We’re going to find a way of choosing Δv1 and Δv2 so as to make ΔC negative</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208110157193.png" alt="image-20200208110157193"></p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208110347567.png" alt="image-20200208110347567"></p>
<p>不用去明白这个符号到底是什么意思，知道他是这样一个向量就行了</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208110733478.png" alt="image-20200208110733478"></p>
<p>But what’s really exciting about the equation is that it lets us see how to choose Δv so as to make ΔC negative. In particular, suppose we choose</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208114023137.png" alt="image-20200208114023137"></p>
<p>Because ∥∇C∥2≥0, this guarantees that ΔC≤0</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208203531050.png" alt="image-20200208203531050"></p>
<p>他这里这个v→v′就相当于v′，Δv等于那么多，v+Δv就是v′</p>
<h3 id="当c由多个变量组成时"><a href="#当c由多个变量组成时" class="headerlink" title="当c由多个变量组成时"></a>当c由多个变量组成时</h3><p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208114652865.png" alt="image-20200208114652865"></p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208114704063.png" alt="image-20200208114704063"></p>
<p>同样得到：</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208114754657.png" alt="image-20200208114754657"></p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208114714851.png" alt="image-20200208114714851"></p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208203531050.png" alt="image-20200208203531050"></p>
<p>注意到最终的cost function 是决定于n次实验的平均值，这将是学习过程变得非常缓慢，因此我们先选取一个大小为m的样本进行训练，缩短学习时间</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208121149748.png" alt="image-20200208121149748"></p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208121229231.png" alt="image-20200208121229231"></p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208121342863.png" alt="image-20200208121342863"></p>
<blockquote>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208121646108.png" alt="image-20200208121646108"></p>
</blockquote>
<p>这部分没有看懂。。。</p>
<ul>
<li>Online，on-line，or incremental learaning ——每次训练一个数据</li>
</ul>
<p>wjk is the weight for the connection between the kth neuron in the second layer, and the jth neuron in the third layer. </p>
<p>This ordering of the j and k indices may seem strange - surely it’d make more sense to swap the j and k indices around? The big advantage of using this ordering is that it means that the vector of activations of the third layer of neurons is:</p>
<p><img src= "/img/loading.gif" data-src="C:%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200208161622964.png" alt="image-20200208161622964"></p>
<hr>
<h1 id="操作方法"><a href="#操作方法" class="headerlink" title="操作方法"></a>操作方法</h1><ol>
<li>network.py代码</li>
<li>data_load.py代码</li>
<li>命令行cd到文件夹内，输入python转到python shell模式，载入程序和文件</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> data_loader</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = data_loader.load_data_wrapper()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> network</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>先建立起一个有30个隐藏神经元的network</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])//第一层<span class="number">784</span>个，第二层<span class="number">30</span>个，第三层<span class="number">10</span>个</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>用随机梯度下降法从 <code>training_data</code> 训练30个epoch, 每个mini-batch含有10个数据，η=3.0</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>

<p>​    跑起来得等一会，相加速可以减少epch数、减少隐藏神经元数、减少数据集量。如果优化一下这个跑起来都会快很多</p>
<p>​    显示内容类似如下（因为是随机的所以数据肯定不一样）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: 9129 &#x2F; 10000</span><br><span class="line">Epoch 1: 9295 &#x2F; 10000</span><br><span class="line">Epoch 2: 9348 &#x2F; 10000</span><br><span class="line">...</span><br><span class="line">Epoch 27: 9528 &#x2F; 10000</span><br><span class="line">Epoch 28: 9542 &#x2F; 10000</span><br><span class="line">Epoch 29: 9534 &#x2F; 10000</span><br></pre></td></tr></table></figure>

<p>可见正确率还挺高</p>
<p>可以随便改一改参数让他以各种姿势跑</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">100</span>, <span class="number">10</span>])//<span class="number">100</span>个隐藏单元</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>

<p>准确率可以更高</p>
<p>关于η</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">100</span>, <span class="number">10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">0.001</span>, test_data=test_data)</span><br></pre></td></tr></table></figure>

<p>结果会差很多</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: 1139 &#x2F; 10000</span><br><span class="line">Epoch 1: 1136 &#x2F; 10000</span><br><span class="line">Epoch 2: 1135 &#x2F; 10000</span><br><span class="line">...</span><br><span class="line">Epoch 27: 2101 &#x2F; 10000</span><br><span class="line">Epoch 28: 2123 &#x2F; 10000</span><br><span class="line">Epoch 29: 2142 &#x2F; 10000</span><br></pre></td></tr></table></figure>

<p><strong>In general, debugging a neural network can be challenging. This is especially true when the initial choice of hyper-parameters produces results no better than random noise.</strong> Suppose we try the successful 30 hidden neuron network architecture from earlier, but with the learning rate changed to η=100.0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; net &#x3D; network.Network([784, 30, 10])</span><br><span class="line">&gt;&gt;&gt; net.SGD(training_data, 30, 10, 100.0, test_data&#x3D;test_data)</span><br></pre></td></tr></table></figure>

<p>At this point we’ve actually gone too far, and the learning rate is too high:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: 1009 &#x2F; 10000</span><br><span class="line">Epoch 1: 1009 &#x2F; 10000</span><br><span class="line">Epoch 2: 1009 &#x2F; 10000</span><br><span class="line">Epoch 3: 1009 &#x2F; 10000</span><br><span class="line">...</span><br><span class="line">Epoch 27: 982 &#x2F; 10000</span><br><span class="line">Epoch 28: 982 &#x2F; 10000</span><br><span class="line">Epoch 29: 982 &#x2F; 10000</span><br></pre></td></tr></table></figure>

<p>Now imagine that we were coming to this problem for the first time. Of course, we <em>know</em> from our earlier experiments that the right thing to do is to decrease the learning rate. <strong>But if we were coming to this problem for the first time then there wouldn’t be much in the output to guide us on what to do. We might worry not only about the learning rate, but about every other aspect of our neural network. We might wonder if we’ve initialized the weights and biases in a way that makes it hard for the network to learn? Or maybe we don’t have enough training data to get meaningful learning? Perhaps we haven’t run for enough epochs? Or maybe it’s impossible for a neural network with this architecture to learn to recognize handwritten digits? Maybe the learning rate is too <em>low</em>? Or, maybe, the learning rate is too high? When you’re coming to a problem for the first time, you’re not always sure.</strong></p>
<p>预知到底怎么调参数，还得再研究</p>
<p>其他：</p>
<p>nn是我的文件夹 dp是人家的文件夹</p>
<p>如果有必要的话可以研究一下命令行，为什么我的批处理文件跑不起来，这样就不用每次输一堆导入的</p>
<p>当然仅限于有必要，毕竟这个东西就是拿着上上手，后面的什么优化函数我都没看</p>
<h1 id="什么是端到端学习？"><a href="#什么是端到端学习？" class="headerlink" title="什么是端到端学习？"></a>什么是端到端学习？</h1><p>端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。但是一般情况下，更常见的是介于两者之间的解决思路，先把任务拆解为简单的两步解决。例如人脸识别门禁系统，不是由检测到的图片直接得到结果（识别出该员工的id），而是第一步将检测到的图片中的人脸部分放大居中，第二步通过识别得到该员工的id。</p>
<p>就是不要预处理和特征提取，直接把原始数据扔进去得到最终结果</p>
<p><img src= "/img/loading.gif" data-src="E:%5CTypora%5C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.assets%5C20180611213203230.png" alt="img"></p>
<h1 id="Softmax回归介绍"><a href="#Softmax回归介绍" class="headerlink" title="Softmax回归介绍"></a>Softmax回归介绍</h1></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">熊猫熬夜不伤神</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/02/04/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">http://yoursite.com/2020/02/04/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com" target="_blank">纸箱小站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="https://wx2.sbimg.cn/2020/07/29/PJFqV.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/02/10/Git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><img class="prev-cover" data-src="https://ftp.bmp.ovh/imgs/2020/07/010ccb989ca14eeb.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Git汇总</div></div></a></div><div class="next-post pull-right"><a href="/2020/01/28/hello%20typora/"><img class="next-cover" data-src="https://ftp.bmp.ovh/imgs/2020/07/010ccb989ca14eeb.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">markdown试水</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 熊猫熬夜不伤神</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script></body></html>